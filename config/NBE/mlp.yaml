# Hydra configuration for MLP training

# Data settings
train_dir: /workspace/dataset/bin/toy_all_model/train
val_dir: /workspace/dataset/bin/toy_all_model/valid
node_id: 1
global_normalize: True

preload: false
glob: "*.feather"
alpha: 3.0

# Dataloader
batch_size: 128
num_workers: 2

# Model
hidden_size: 128
num_layers: 9
dropout: 0.0
output_size: null

# Optimization
lr: 0.001
weight_decay: 0.0
epochs: 100
loss_weight_alpha: 10.0
loss_weight_gamma: 3.0
loss_weight_time_beta: 10.0

# LR / early stopping
lr_patience: 10
lr_factor: 0.5
min_lr: 1e-6
max_lr_reductions: 5
early_stop_patience: 15

# Misc
save_dir: NBE/MLP_alpha3
seed: 42
